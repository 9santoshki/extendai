# ===========================================
# AI Agent Backend Configuration
# Custom OpenAI-Compatible API
# ===========================================

# Local Model Configuration
# API key can be any value for local models
OPENAI_API_KEY=ollama

# Model names (adjust versions if needed)
# Chat model for agent interactions
CHAT_MODEL=qwen2.5:0.5b
# Reasoning model for analytical tasks
REASONING_MODEL=qwen2.5:7b

# Custom API endpoint for LLM service
BASE_URL=http://localhost:11434/v1

# Server Configuration
HOST=0.0.0.0
PORT=8001

# Logging
LOG_LEVEL=INFO

# ===========================================
# Instructions:
# ===========================================
# 1. Make sure Ollama is running:
#    $ ollama serve
#
# 2. Verify models are installed:
#    $ ollama list
#    If not, install them:
#    $ ollama pull qwen2.5:0.5b
#    $ ollama pull qwen2.5:7b
#
# 3. Start the backend:
#    $ ./start.sh
#
# 4. Test it:
#    $ curl http://localhost:8000/health
# ===========================================
